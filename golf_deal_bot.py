#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³¨í”„ì¥ íˆ¬ì ë° M&A ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ í…”ë ˆê·¸ë¨ ë´‡
ì‘ì„±ì¼: 2026-01-10
ìˆ˜ì •ì¼: 2026-01-13

[ì¤‘ë³µ ë‰´ìŠ¤ ìµœì†Œí™” ê°œì„  ì‚¬í•­]
1. ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„ ë‹¨ì¶•: 48ì‹œê°„ â†’ 12ì‹œê°„ (ì¤‘ë³µ ìˆ˜ì§‘ í¬ê²Œ ê°ì†Œ)
2. ì œëª©/URL ì •ê·œí™”: ìœ ì‚¬í•œ ì œëª©ê³¼ URL ë³€í˜•ë„ ì¤‘ë³µìœ¼ë¡œ ê°ì§€
3. íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê´€ë¦¬: ì „ì†¡ ê¸°ë¡ì— ì‹œê°„ ì •ë³´ ì¶”ê°€
4. ìë™ ì •ë¦¬ ê¸°ëŠ¥: 7ì¼ ì´ìƒ ì˜¤ë˜ëœ ê¸°ë¡ ìë™ ì‚­ì œ (íŒŒì¼ ë¹„ëŒ€í™” ë°©ì§€)
"""

import requests  # HTTP ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í…”ë ˆê·¸ë¨ API í˜¸ì¶œ ë° RSS ê°€ì ¸ì˜¤ê¸°)
import time  # ì‹œê°„ ê´€ë ¨ í•¨ìˆ˜ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime, timedelta, timezone  # ë‚ ì§œ/ì‹œê°„ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.parse import quote, urlparse  # URL ì¸ì½”ë”© ë° íŒŒì‹±ì„ ìœ„í•œ í•¨ìˆ˜
import hashlib  # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±
import xml.etree.ElementTree as ET  # XML íŒŒì‹±ì„ ìœ„í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (RSS íŒŒì‹±ìš©)
from email.utils import parsedate_to_datetime  # RFC 2822 ë‚ ì§œ íŒŒì‹±ìš©
import re  # ì •ê·œí‘œí˜„ì‹ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì œëª© ì •ê·œí™”ìš©)

# ==================== ì„¤ì • êµ¬ê°„ (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!) ====================
TELEGRAM_BOT_TOKEN = "8180938946:AAH3gZS6uNTsFAUwdJh2rvgMC4_QmYUAZkw"  # ì—¬ê¸°ì— í…”ë ˆê·¸ë¨ ë´‡ í† í°ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: "1234567890:ABCdefGHIjklMNOpqrsTUVwxyz")
TELEGRAM_CHAT_ID = "143110040"    # ì—¬ê¸°ì— í…”ë ˆê·¸ë¨ ì±„íŒ… IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: "123456789")
# ========================================================================

# ì¤‘ë³µ ì „ì†¡ ë°©ì§€ë¥¼ ìœ„í•œ íŒŒì¼ëª…
SENT_NEWS_FILE = "sent_news.txt"

# ì¤‘ë³µ ë‰´ìŠ¤ ì¶”ì  ê¸°ê°„ (ì¼ ë‹¨ìœ„) - ì´ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ ê¸°ë¡ì€ ì‚­ì œë¨
SENT_NEWS_RETENTION_DAYS = 7

# ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„) - ë” ì§§ì€ ì‹œê°„ìœ¼ë¡œ ìµœê·¼ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
NEWS_FETCH_HOURS = 12

# í•œêµ­ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
KOREAN_KEYWORDS = [
    "ê³¨í”„ì¥",
    "ë§¤ê°",
    "ì¸ìˆ˜",
    "MBK",
    "ì¹´ì¹´ì˜¤vx",
    "ìŠ¤ë§ˆíŠ¸ìŠ¤ì½”ì–´",
    "ì„¼íŠ¸ë¡œì´ë“œ",
    "ì´ë„",
    "QED",
    "M&A",
    "IPO",
    "ì‚¬ëª¨í€ë“œ",
    "PEF"
]

# ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
ENGLISH_KEYWORDS = [
    "Golf course acquisition",
    "Golf course M&A",
    "Golf course IPO",
    "Private Equity Golf",
    "Golf investment deal"
]

# ì§„ì„± ë‰´ìŠ¤ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ í•„ìˆ˜ í‚¤ì›Œë“œ (ì´ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ì•¼ í•¨)
FILTER_KEYWORDS = [
    # í•œêµ­ì–´ - íˆ¬ì/M&A ê´€ë ¨
    "ì¸ìˆ˜", "ë§¤ê°", "íˆ¬ì", "ë”œ", "ì‚¬ëª¨í€ë“œ", "PEF", "M&A", "IPO", "ìƒì¥",
    # í•œêµ­ì–´ - ê°œë°œ/ê²½ì˜ ê´€ë ¨
    "ê°œë°œ", "ì™„ì„±", "ì˜¤í”ˆ", "ê°œì¥", "ë¶„ì–‘", "íšŒì›ê¶Œ", "ì¬ì •ë¹„", "ë¦¬ëª¨ë¸ë§", "ì¡°ì„±",
    # ì˜ì–´
    "acquisition", "deal", "capital", "investment", "private equity",
    "merger", "IPO", "buyout", "acquire", "purchase", "development", "opening"
]

# ì œì™¸í•  ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ (ì´ê²ƒì´ í¬í•¨ë˜ë©´ ì œì™¸)
EXCLUDE_KEYWORDS = [
    "ëŒ€íšŒ", "í† ë„ˆë¨¼íŠ¸", "ë ˆìŠ¨", "ìš°ìŠ¹", "ì±”í”¼ì–¸ì‹­", "íƒ€ìˆ˜", "ìŠ¤ìœ™",
    "tournament", "championship", "lesson", "winner", "score", "swing"
]


def load_sent_news():
    """
    ì´ë¯¸ ì „ì†¡í•œ ë‰´ìŠ¤ ëª©ë¡ì„ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ë°˜í™˜: ì „ì†¡í•œ ë‰´ìŠ¤ í•´ì‹œê°’ë“¤ì˜ ë”•ì…”ë„ˆë¦¬ {í•´ì‹œ: íƒ€ì„ìŠ¤íƒ¬í”„}
    """
    try:
        sent_news = {}
        with open(SENT_NEWS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” ê²½ìš° (ìƒˆ í˜•ì‹: hash|timestamp)
                if '|' in line:
                    parts = line.split('|')
                    news_hash = parts[0]
                    timestamp = float(parts[1]) if len(parts) > 1 else time.time()
                else:
                    # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ëŠ” ê²½ìš° (êµ¬ í˜•ì‹: hashë§Œ)
                    news_hash = line
                    timestamp = time.time()  # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
                sent_news[news_hash] = timestamp
        return sent_news
    except FileNotFoundError:
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        return {}


def save_sent_news(news_hash):
    """
    ì „ì†¡í•œ ë‰´ìŠ¤ì˜ í•´ì‹œê°’ì„ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ íŒŒì¼ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    ë§¤ê°œë³€ìˆ˜: news_hash - ë‰´ìŠ¤ë¥¼ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ëŠ” í•´ì‹œê°’
    """
    with open(SENT_NEWS_FILE, 'a', encoding='utf-8') as f:
        # í•´ì‹œê°’ê³¼ í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ íŒŒì¼ ëì— ì¶”ê°€
        timestamp = time.time()
        f.write(f"{news_hash}|{timestamp}\n")


def cleanup_old_sent_news(sent_news):
    """
    ì˜¤ë˜ëœ ë‰´ìŠ¤ ê¸°ë¡ì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
    ë§¤ê°œë³€ìˆ˜: sent_news - ë‰´ìŠ¤ í•´ì‹œì™€ íƒ€ì„ìŠ¤íƒ¬í”„ ë”•ì…”ë„ˆë¦¬
    """
    now = time.time()
    retention_seconds = SENT_NEWS_RETENTION_DAYS * 24 * 60 * 60

    # ìœ ì§€í•  ë‰´ìŠ¤ë§Œ í•„í„°ë§
    cleaned_news = {
        hash_val: timestamp
        for hash_val, timestamp in sent_news.items()
        if now - timestamp < retention_seconds
    }

    # ì •ë¦¬ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ íŒŒì¼ì„ ë‹¤ì‹œ ì‘ì„±
    if len(cleaned_news) < len(sent_news):
        removed_count = len(sent_news) - len(cleaned_news)
        print(f"ğŸ—‘ï¸ {removed_count}ê°œì˜ ì˜¤ë˜ëœ ë‰´ìŠ¤ ê¸°ë¡ ì‚­ì œ (ë³´ê´€ ê¸°ê°„: {SENT_NEWS_RETENTION_DAYS}ì¼)")

        with open(SENT_NEWS_FILE, 'w', encoding='utf-8') as f:
            for hash_val, timestamp in cleaned_news.items():
                f.write(f"{hash_val}|{timestamp}\n")

    return cleaned_news


def normalize_title(title):
    """
    ì œëª©ì„ ì •ê·œí™”í•˜ì—¬ ìœ ì‚¬í•œ ì œëª©ì˜ ì¤‘ë³µì„ ë°©ì§€í•˜ëŠ” í•¨ìˆ˜
    ë§¤ê°œë³€ìˆ˜: title - ì›ë³¸ ì œëª©
    ë°˜í™˜: ì •ê·œí™”ëœ ì œëª©
    """
    # ì†Œë¬¸ìë¡œ ë³€í™˜
    normalized = title.lower()
    # íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì •ê·œí™” (ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ)
    normalized = re.sub(r'\s+', ' ', normalized)
    # ë”°ì˜´í‘œ ì œê±°
    normalized = re.sub(r'["\']', '', normalized)
    # ì•ë’¤ ê³µë°± ì œê±°
    normalized = normalized.strip()
    return normalized


def normalize_url(url):
    """
    URLì„ ì •ê·œí™”í•˜ì—¬ ìœ ì‚¬í•œ URLì˜ ì¤‘ë³µì„ ë°©ì§€í•˜ëŠ” í•¨ìˆ˜
    ë§¤ê°œë³€ìˆ˜: url - ì›ë³¸ URL
    ë°˜í™˜: ì •ê·œí™”ëœ URL
    """
    try:
        # URL íŒŒì‹±
        parsed = urlparse(url)
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°í•˜ê³  ë„ë©”ì¸ + ê²½ë¡œë§Œ ì‚¬ìš©
        # (êµ¬ê¸€ ë‰´ìŠ¤ëŠ” ê°™ì€ ê¸°ì‚¬ë¥¼ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë¡œ ì œê³µí•  ìˆ˜ ìˆìŒ)
        normalized = f"{parsed.netloc}{parsed.path}"
        return normalized.lower()
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return url.lower()


def generate_news_hash(title, link):
    """
    ë‰´ìŠ¤ì˜ ê³ ìœ  í•´ì‹œê°’ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì¤‘ë³µ ì²´í¬ìš©)
    ë§¤ê°œë³€ìˆ˜: title - ë‰´ìŠ¤ ì œëª©, link - ë‰´ìŠ¤ ë§í¬
    ë°˜í™˜: MD5 í•´ì‹œê°’ (ë¬¸ìì—´)
    """
    # ì œëª©ê³¼ ë§í¬ë¥¼ ì •ê·œí™”
    normalized_title = normalize_title(title)
    normalized_link = normalize_url(link)

    # ì •ê·œí™”ëœ ì œëª©ê³¼ ë§í¬ë¥¼ í•©ì³ì„œ ìœ ë‹ˆí¬í•œ ë¬¸ìì—´ ìƒì„±
    unique_string = f"{normalized_title}|{normalized_link}"
    # MD5 í•´ì‹œë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    return hashlib.md5(unique_string.encode('utf-8')).hexdigest()


def is_relevant_news(title, summary):
    """
    ë‰´ìŠ¤ê°€ ê³¨í”„ íˆ¬ì/M&A ê´€ë ¨ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
    ë§¤ê°œë³€ìˆ˜: title - ë‰´ìŠ¤ ì œëª©, summary - ë‰´ìŠ¤ ìš”ì•½
    ë°˜í™˜: True (ê´€ë ¨ ë‰´ìŠ¤) / False (ë…¸ì´ì¦ˆ)
    """
    # ì œëª©ê³¼ ìš”ì•½ì„ ì†Œë¬¸ìë¡œ ë³€í™˜ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰í•˜ê¸° ìœ„í•´)
    text = (title + ' ' + summary).lower()

    # ë¨¼ì € ì œì™¸ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    for exclude in EXCLUDE_KEYWORDS:
        if exclude.lower() in text:
            # ë…¸ì´ì¦ˆ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´ False ë°˜í™˜
            return False

    # í•„ìˆ˜ í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    for keyword in FILTER_KEYWORDS:
        if keyword.lower() in text:
            # ê´€ë ¨ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´ True ë°˜í™˜
            return True

    # ì–´ë–¤ í‚¤ì›Œë“œë„ ì—†ìœ¼ë©´ False ë°˜í™˜
    return False


def send_telegram_message(title, link):
    """
    í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜
    ë§¤ê°œë³€ìˆ˜: title - ë‰´ìŠ¤ ì œëª©, link - ë‰´ìŠ¤ ë§í¬
    ë°˜í™˜: True (ì „ì†¡ ì„±ê³µ) / False (ì „ì†¡ ì‹¤íŒ¨)
    """
    # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í˜•ì‹ ì‘ì„± (ê°„ê²°í•œ í¬ë§·: ì œëª© + ë§í¬)
    message = f"""ğŸŒï¸ {title}

{link}"""

    # í…”ë ˆê·¸ë¨ API URL
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    # ì „ì†¡í•  ë°ì´í„°
    payload = {
        'chat_id': TELEGRAM_CHAT_ID,
        'text': message,
        'disable_web_page_preview': False  # ë§í¬ ë¯¸ë¦¬ë³´ê¸° í™œì„±í™”
    }

    try:
        # HTTP POST ìš”ì²­ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
        response = requests.post(url, data=payload, timeout=10)

        # ì‘ë‹µ ì½”ë“œê°€ 200ì´ë©´ ì„±ê³µ
        if response.status_code == 200:
            print(f"âœ… ì „ì†¡ ì™„ë£Œ: {title}")
            return True
        else:
            print(f"âŒ ì „ì†¡ ì‹¤íŒ¨ (ì½”ë“œ {response.status_code}): {title}")
            return False

    except Exception as e:
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ í”„ë¡œê·¸ë¨ì€ ê³„ì† ì‹¤í–‰
        print(f"âš ï¸ ì „ì†¡ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return False


def fetch_google_news(keyword):
    """
    êµ¬ê¸€ ë‰´ìŠ¤ RSSì—ì„œ íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    ë§¤ê°œë³€ìˆ˜: keyword - ê²€ìƒ‰í•  í‚¤ì›Œë“œ
    ë°˜í™˜: ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸
    """
    # í‚¤ì›Œë“œë¥¼ URL ì¸ì½”ë”© (í•œê¸€, íŠ¹ìˆ˜ë¬¸ì ë“±ì„ URLì— ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë³€í™˜)
    encoded_keyword = quote(keyword)

    # êµ¬ê¸€ ë‰´ìŠ¤ RSS URL (ìµœê·¼ ë‰´ìŠ¤ ê¸°ì¤€)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"

    try:
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {keyword}")

        # RSS í”¼ë“œë¥¼ HTTP GET ìš”ì²­ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        response = requests.get(rss_url, timeout=10)
        response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ

        # XML íŒŒì‹±
        root = ET.fromstring(response.content)

        # í˜„ì¬ ì‹œê°„ (UTC ê¸°ì¤€)
        now = datetime.now(timezone.utc)
        # ì„¤ì •ëœ ì‹œê°„(ê¸°ë³¸ 12ì‹œê°„) ì´ì „ ì‹œê°„
        cutoff_time = now - timedelta(hours=NEWS_FETCH_HOURS)

        # ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±
        entries = []

        # RSS 2.0 í˜•ì‹: channel/item íƒœê·¸ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ
        for item in root.findall('.//item'):
            # ì œëª© ì¶”ì¶œ
            title_elem = item.find('title')
            title = title_elem.text if title_elem is not None else 'ì œëª© ì—†ìŒ'

            # ë§í¬ ì¶”ì¶œ
            link_elem = item.find('link')
            link = link_elem.text if link_elem is not None else ''

            # ìš”ì•½(ì„¤ëª…) ì¶”ì¶œ
            desc_elem = item.find('description')
            summary = desc_elem.text if desc_elem is not None else 'ìš”ì•½ ì—†ìŒ'

            # ë°œí–‰ì¼ ì¶”ì¶œ
            pub_date_elem = item.find('pubDate')
            if pub_date_elem is not None and pub_date_elem.text:
                try:
                    # RFC 2822 í˜•ì‹ì˜ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                    pub_date = parsedate_to_datetime(pub_date_elem.text)

                    # ì„¤ì •ëœ ì‹œê°„ ì´ë‚´ì˜ ê¸°ì‚¬ë§Œ í¬í•¨
                    if pub_date < cutoff_time:
                        continue  # ì˜¤ë˜ëœ ê¸°ì‚¬ëŠ” ê±´ë„ˆë›°ê¸°
                except Exception:
                    # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í¬í•¨
                    pass

            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
            entries.append({
                'title': title,
                'link': link,
                'summary': summary
            })

        # ê°€ì ¸ì˜¨ ë‰´ìŠ¤ ê°œìˆ˜ ì¶œë ¥
        print(f"   â””â”€ {len(entries)}ê°œ ë‰´ìŠ¤ ë°œê²¬ (ìµœê·¼ {NEWS_FETCH_HOURS}ì‹œê°„ ì´ë‚´)")

        # ë‰´ìŠ¤ í•­ëª© ë°˜í™˜
        return entries

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
        print(f"âš ï¸ '{keyword}' ê²€ìƒ‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        print("   â””â”€ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤...")
        return []


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 60)
    print("ğŸŒï¸ ê³¨í”„ ë”œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë´‡ ì‹œì‘")
    print("=" * 60)

    # í…”ë ˆê·¸ë¨ ì„¤ì • í™•ì¸
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("âŒ ì˜¤ë¥˜: TELEGRAM_BOT_TOKENê³¼ TELEGRAM_CHAT_IDë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return

    # ì´ë¯¸ ì „ì†¡í•œ ë‰´ìŠ¤ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    sent_news = load_sent_news()
    print(f"ğŸ“ ì´ë¯¸ ì „ì†¡ëœ ë‰´ìŠ¤: {len(sent_news)}ê°œ")

    # ì˜¤ë˜ëœ ë‰´ìŠ¤ ê¸°ë¡ ì •ë¦¬
    sent_news = cleanup_old_sent_news(sent_news)
    print()

    # ëª¨ë“  í‚¤ì›Œë“œ í•©ì¹˜ê¸°
    all_keywords = KOREAN_KEYWORDS + ENGLISH_KEYWORDS

    # ìˆ˜ì§‘ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_news = []

    # ê° í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    total_fetched = 0
    total_duplicates = 0
    total_filtered = 0

    for keyword in all_keywords:
        # êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        entries = fetch_google_news(keyword)
        total_fetched += len(entries)

        # ê° ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬
        for entry in entries:
            try:
                # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ
                title = entry.get('title', 'ì œëª© ì—†ìŒ')
                link = entry.get('link', '')
                summary = entry.get('summary', 'ìš”ì•½ ì—†ìŒ')

                # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±
                news_hash = generate_news_hash(title, link)

                # ì´ë¯¸ ì „ì†¡í•œ ë‰´ìŠ¤ë©´ ê±´ë„ˆë›°ê¸°
                if news_hash in sent_news:
                    total_duplicates += 1
                    continue

                # ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ í•„í„°ë§
                if not is_relevant_news(title, summary):
                    total_filtered += 1
                    print(f"   âŠ— í•„í„°ë§ë¨: {title[:50]}...")
                    continue

                # ì¡°ê±´ì„ í†µê³¼í•œ ë‰´ìŠ¤ ì €ì¥
                all_news.append({
                    'title': title,
                    'link': link,
                    'summary': summary,
                    'hash': news_hash
                })
                print(f"   âœ“ ì„ íƒë¨: {title[:50]}...")

            except Exception as e:
                # ê°œë³„ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ ì‹œ
                print(f"âš ï¸ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                print("   â””â”€ ë‹¤ìŒ ë‰´ìŠ¤ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤...")
                continue

        # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸° (1ì´ˆ)
        time.sleep(1)

    print(f"\nğŸ“Š í•„í„°ë§ ê²°ê³¼:")
    print(f"   - ì´ ìˆ˜ì§‘: {total_fetched}ê°œ")
    print(f"   - ì¤‘ë³µ ì œì™¸: {total_duplicates}ê°œ")
    print(f"   - í•„í„°ë§ë¨: {total_filtered}ê°œ")
    print(f"   - ìµœì¢… ì„ íƒ: {len(all_news)}ê°œ\n")

    # ì „ì†¡ ì¹´ìš´í„°
    success_count = 0

    # í•„í„°ë§ëœ ë‰´ìŠ¤ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡
    if all_news:
        print("ğŸ“¤ í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹œì‘...\n")

        for news in all_news:
            # í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡
            if send_telegram_message(news['title'], news['link']):
                # ì „ì†¡ ì„±ê³µ ì‹œ íŒŒì¼ì— ì €ì¥
                save_sent_news(news['hash'])
                success_count += 1

            # í…”ë ˆê·¸ë¨ API ì œí•œ ë°©ì§€ (ë©”ì‹œì§€ ê°„ 1ì´ˆ ëŒ€ê¸°)
            time.sleep(1)
    else:
        print("â„¹ï¸ ì „ì†¡í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(f"âœ¨ ì‘ì—… ì™„ë£Œ!")
    print(f"   - ì „ì†¡ ì„±ê³µ: {success_count}ê°œ")
    print(f"   - ì „ì†¡ ì‹¤íŒ¨: {len(all_news) - success_count}ê°œ")
    print("=" * 60)


# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œì—ë§Œ main() í•¨ìˆ˜ í˜¸ì¶œ
if __name__ == "__main__":
    main()
